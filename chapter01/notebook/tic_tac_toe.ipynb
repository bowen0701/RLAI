{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMARKS = 3\n",
    "BOARD_NROWS = BOARD_NCOLS = 3\n",
    "BOARD_SIZE = BOARD_NROWS * BOARD_NCOLS\n",
    "\n",
    "CROSS = 1\n",
    "CIRCLE = -1\n",
    "EMPTY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash(board):\n",
    "    return ','.join([str(x) for x in list(board.reshape(BOARD_SIZE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        self.steps_left = BOARD_SIZE\n",
    "        self.board = (np.array([EMPTY] * BOARD_SIZE)\n",
    "                        .reshape((BOARD_NROWS, BOARD_NCOLS)))\n",
    "        self.state = hash(self.board)\n",
    "        self.winner = EMPTY\n",
    "\n",
    "    def get_positions(self):\n",
    "        \"\"\"Get possible action positions given current board.\"\"\"\n",
    "        positions = []\n",
    "        for r in range(BOARD_NROWS):\n",
    "            for c in range(BOARD_NCOLS):\n",
    "                if self.board[r][c] == EMPTY:\n",
    "                    positions.append((r, c))\n",
    "        return positions\n",
    "\n",
    "    def _judge(self):\n",
    "        \"\"\"Judge winner based on the current board.\"\"\"\n",
    "        # Check rows.\n",
    "        for r in range(BOARD_NROWS):\n",
    "            row = self.board[r, :]\n",
    "            symbol = row[0]\n",
    "            if symbol != EMPTY and np.sum(row) == symbol * NMARKS:\n",
    "                self.winner = symbol\n",
    "                self.steps_left = 0\n",
    "                return self\n",
    "\n",
    "        # Check columns.\n",
    "        for c in range(BOARD_NCOLS):\n",
    "            col = self.board[:, c]\n",
    "            symbol = col[0]\n",
    "            if symbol != EMPTY and np.sum(col) == symbol * NMARKS:\n",
    "                self.winner = symbol\n",
    "                self.steps_left = 0\n",
    "                return self\n",
    "\n",
    "        # Check diagonals.\n",
    "        mid = BOARD_NROWS // 2\n",
    "        symbol = self.board[mid][mid]\n",
    "        if symbol != EMPTY: \n",
    "            diag1, diag2 = [], []\n",
    "            for i in range(BOARD_NROWS):\n",
    "                diag1.append(self.board[i][i])\n",
    "                diag2.append(self.board[i][BOARD_NROWS - i - 1])\n",
    "\n",
    "            diag1, diag2 = np.array(diag1), np.array(diag2)\n",
    "            if (np.sum(diag1) == symbol * NMARKS or \n",
    "                np.sum(diag2) == symbol * NMARKS):\n",
    "                self.winner = symbol\n",
    "                self.steps_left = 0\n",
    "                return self\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"Check the game is done.\"\"\"\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def step(self, r, c, symbol):\n",
    "        \"\"\"Take a step with symbol.\"\"\"\n",
    "        env_next = self._copy()\n",
    "        env_next.board[r][c] = symbol\n",
    "        env_next.state = hash(env_next.board)\n",
    "        env_next.steps_left -= 1\n",
    "        env_next._judge()\n",
    "        return env_next\n",
    "\n",
    "    def _copy(self):\n",
    "        \"\"\"Copy to a new Environment instance.\"\"\"\n",
    "        env_copy = Environment()\n",
    "        env_copy.steps_left = self.steps_left\n",
    "        env_copy.board = self.board.copy()\n",
    "        env_copy.state = self.state\n",
    "        env_copy.winner = self.winner\n",
    "        return env_copy\n",
    "\n",
    "    def show_board(self):\n",
    "        \"\"\"Show board.\"\"\"\n",
    "        board = self.board.tolist()\n",
    "        for r in range(BOARD_NROWS):\n",
    "            for c in range(BOARD_NCOLS):\n",
    "                if board[r][c] == CROSS:\n",
    "                    board[r][c] = 'X'\n",
    "                elif board[r][c] == CIRCLE:\n",
    "                    board[r][c] = 'O'\n",
    "                else:\n",
    "                    board[r][c] = ' '\n",
    "\n",
    "        print('Board: is_done={}, steps_left={}, winner={}'\n",
    "              .format(self.is_done(), self.steps_left, self.winner))\n",
    "        for r in range(BOARD_NROWS):\n",
    "            print(board[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dfs_states(cur_symbol, env, all_state_env_d):\n",
    "    \"\"\"DFS for next state by recursion.\"\"\"\n",
    "    for r in range(BOARD_NROWS):\n",
    "        for c in range(BOARD_NCOLS):\n",
    "            if env.board[r][c] == EMPTY:\n",
    "                env_next = env.step(r, c, cur_symbol)\n",
    "                if env_next.state not in all_state_env_d:\n",
    "                    all_state_env_d[env_next.state] = env_next\n",
    "\n",
    "                    # If game is not ended, continue DFS.\n",
    "                    if not env_next.is_done():\n",
    "                        _dfs_states(-cur_symbol, env_next, all_state_env_d)\n",
    "\n",
    "def get_all_states():\n",
    "    \"\"\"Get all states from the init state.\"\"\"\n",
    "    # The player who plays first always uses 'X'.\n",
    "    cur_symbol = CROSS\n",
    "\n",
    "    # Apply DFS to collect all states.\n",
    "    env = Environment()\n",
    "    all_state_env_d = dict()\n",
    "    all_state_env_d[env.state] = env\n",
    "    _dfs_states(cur_symbol, env, all_state_env_d)\n",
    "    return all_state_env_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_STATE_ENV_D = get_all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Environment()\n",
    "# env.__dict__\n",
    "# print(env.board)\n",
    "# print(env.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = env.step(1, 1, 1)\n",
    "# print(env.board)\n",
    "# print(env.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = env.step(0, 0, -1)\n",
    "# print(env.board)\n",
    "# print(env.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s, env in ALL_STATE_ENV_DICT.items():\n",
    "#     if env.winner == CIRCLE:\n",
    "#         env.show_board()\n",
    "#         print('=>', env.winner, env.is_done(), env.steps_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, player='X', step_size=0.01, epsilon=0.01):\n",
    "        self.player = player\n",
    "        if self.player == 'X':\n",
    "            self.symbol = CROSS\n",
    "        elif self.player == 'O':\n",
    "            self.symbol = CIRCLE\n",
    "        else:\n",
    "            raise InputError(\"Input player should be 'X' or 'O'\")\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Create a state-value table V:state->value.\n",
    "        self.V = dict()\n",
    "\n",
    "        # Memoize action state, its parent state & is_greedy bool:\n",
    "        # state_parent_d:state->parent state & state_isgreedy_d:state->is_greedy bool.\n",
    "        self.states = []\n",
    "        self.state_parent_d = dict()\n",
    "        self.state_isgreedy_d = dict()\n",
    "\n",
    "    def init_state_values(self):\n",
    "        \"\"\"Init state-value table.\"\"\"\n",
    "        all_state_env_d = ALL_STATE_ENV_D\n",
    "\n",
    "        for s, env in all_state_env_d.items():\n",
    "            if env.winner == self.symbol:\n",
    "                self.V[s] = 1.0\n",
    "            elif env.winner == -self.symbol:\n",
    "                self.V[s] = 0.0\n",
    "            else:\n",
    "                self.V[s] = 0.5\n",
    "\n",
    "    def reset_episode(self, env):\n",
    "        \"\"\"Init episode.\"\"\"\n",
    "        self.states.append(env.state)\n",
    "        self.state_parent_d[env.state] = None\n",
    "        self.state_isgreedy_d[env.state] = False\n",
    "\n",
    "    def _play_strategy(self, env, positions):\n",
    "        \"\"\"Play with strategy. Here we use epsilon-greedy strategy.\n",
    "\n",
    "        Epsilon-greedy strategy: \n",
    "          - Take exploratory moves in the p% of times. \n",
    "          - Take greedy moves in the (100-p)% of times.\n",
    "        where p% is epsilon. \n",
    "        If epsilon is zero, then use the greedy strategy.\n",
    "        \"\"\"\n",
    "        # Sort positions based on state-value, by breaking Python sort()'s stability.\n",
    "        vals_positions = []\n",
    "        for (r, c) in positions:\n",
    "            env_next = env.step(r, c, self.symbol)\n",
    "            s = env_next.state\n",
    "            v = self.V[s]\n",
    "            vals_positions.append((v, (r, c)))\n",
    "\n",
    "        np.random.shuffle(vals_positions)\n",
    "        vals_positions.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        p = np.random.random()\n",
    "        if p > self.epsilon:\n",
    "            # Exploit.            \n",
    "            (r_next, c_next) = vals_positions[0][1]\n",
    "            is_greedy = True\n",
    "        else:\n",
    "            # Explore.\n",
    "            if len(vals_positions) > 1:\n",
    "                vals_positions_explore = vals_positions[1:]\n",
    "                n = len(vals_positions_explore)\n",
    "                (r_next, c_next) = vals_positions_explore[np.random.randint(n)][1]\n",
    "            else:\n",
    "                (r_next, c_next) = vals_positions[0][1]\n",
    "            is_greedy = False\n",
    "\n",
    "        env_next = env.step(r_next, c_next, self.symbol)\n",
    "        state_next = env_next.state\n",
    "        return (r_next, c_next, state_next, is_greedy)\n",
    "\n",
    "    def add_state(self, state_next, is_greedy):\n",
    "        state = self.states[-1]\n",
    "        self.state_parent_d[state_next] = state\n",
    "        self.state_isgreedy_d[state_next] = is_greedy\n",
    "        self.states.append(state_next)\n",
    "        return self\n",
    "\n",
    "    def play(self, env):\n",
    "        \"\"\"Play a move from possible states given current state.\"\"\"\n",
    "        # Get next action positions from environment.\n",
    "        positions = env.get_positions()\n",
    "\n",
    "        # Apply epsilon-greedy strategy.\n",
    "        (r_next, c_next, state_next, is_greedy) = self._play_strategy(env, positions)\n",
    "\n",
    "        # Add state.\n",
    "        self.add_state(state_next, is_greedy)\n",
    "        return r_next, c_next, self.symbol\n",
    "\n",
    "    def backup_value(self):\n",
    "        \"\"\"Back up value by a temporal-difference learning after a greedy move.\n",
    "\n",
    "        Temporal-difference learning:\n",
    "          V(S_t) <- V(S_t) + a * [V(S_{t+1}) - V(S_t)]\n",
    "        where a is the step size, and V(S_t) is the state-value function\n",
    "        at time step t.\n",
    "        \"\"\"\n",
    "        s = self.states[-1]\n",
    "        s_prev = self.state_parent_d[s]\n",
    "        is_greedy = self.state_isgreedy_d[s]\n",
    "        if is_greedy:\n",
    "            self.V[s_prev] += self.step_size * (self.V[s] - self.V[s_prev])\n",
    "\n",
    "    def save_state_values(self):\n",
    "        \"\"\"Save learned state-value table.\"\"\"\n",
    "        if self.symbol == CROSS:\n",
    "            json.dump(self.V, open(\"state_value_x.json\", 'w'))\n",
    "        else:\n",
    "            json.dump(self.V, open(\"state_value_o.json\", 'w'))\n",
    "\n",
    "    def load_state_values(self):\n",
    "        \"\"\"Load learned state-value table.\"\"\"\n",
    "        if self.symbol == CROSS:\n",
    "            self.V = json.load(open(\"state_value_x.json\"))\n",
    "        else:\n",
    "            self.V = json.load(open(\"state_value_o.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent1 = Agent(player='X', step_size=0.01, epsilon=0.01)\n",
    "# agent1.reset_episode(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_train(epochs=100000, step_size=0.1, epsilon=0.1, print_per_epochs=100):\n",
    "    \"\"\"Self train an agent by playing games against itself.\"\"\"\n",
    "    agent1 = Agent(player='X', step_size=step_size, epsilon=epsilon)\n",
    "    agent2 = Agent(player='O', step_size=step_size, epsilon=epsilon)\n",
    "    agent1.init_state_values()\n",
    "    agent2.init_state_values()\n",
    "\n",
    "    n_agent1_wins = 0\n",
    "    n_agent2_wins = 0\n",
    "    n_ties = 0\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        # Reset both agents after epoch was done.\n",
    "        env = Environment()\n",
    "        agent1.reset_episode(env)\n",
    "        agent2.reset_episode(env)\n",
    "\n",
    "        while not env.is_done():\n",
    "            # Agent 1 plays one step.\n",
    "            r1, c1, symbol1 = agent1.play(env)\n",
    "            env = env.step(r1, c1, symbol1)\n",
    "            agent1.backup_value()\n",
    "\n",
    "            if env.is_done():\n",
    "                break\n",
    "\n",
    "            # Agent 2 plays the next step.\n",
    "            r2, c2, symbol2 = agent2.play(env)\n",
    "            env = env.step(r2, c2, symbol2)\n",
    "            agent2.backup_value()\n",
    "\n",
    "        # Set final state with is_greedy=True to backup value.\n",
    "        is_greedy = True\n",
    "        if env.winner == CROSS:\n",
    "            agent2.add_state(env.state, is_greedy)\n",
    "            agent2.backup_value()\n",
    "            n_agent1_wins += 1\n",
    "        elif env.winner == CIRCLE:\n",
    "            agent1.add_state(env.state, is_greedy)\n",
    "            agent1.backup_value()\n",
    "            n_agent2_wins += 1\n",
    "        else:\n",
    "            n_ties += 1\n",
    "\n",
    "        # Print board.\n",
    "        if i % print_per_epochs == 0:\n",
    "            print('Epoch {}: Agent1 wins {}, Agent2 wins {}, ties {}'\n",
    "                  .format(i,\n",
    "                          round(n_agent1_wins / i, 2), \n",
    "                          round(n_agent2_wins / i, 2), \n",
    "                          round(n_ties / i, 2)))\n",
    "            env.show_board()\n",
    "            print('---')\n",
    "\n",
    "    agent1.save_state_values()\n",
    "    agent2.save_state_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_train(epochs=100000, step_size=0.1, epsilon=0.075, print_per_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human:\n",
    "    \"\"\"Human class.\"\"\"\n",
    "    def __init__(self, player='X'):\n",
    "        self.player = player\n",
    "        if self.player == 'X':\n",
    "            self.symbol = CROSS\n",
    "        elif self.player == 'O':\n",
    "            self.symbol = CIRCLE\n",
    "\n",
    "    def play(self, env):\n",
    "        \"\"\"Play a move from possible states given current state.\"\"\"\n",
    "        # Get human input position.\n",
    "        input_position_re = re.compile('^[0-2],[0-2]$')\n",
    "        positions = set(env.get_positions())\n",
    "        while True:\n",
    "            input_position = input(\n",
    "                'Please input position for {} in the format: \"row,col\" with '\n",
    "                .format(self.player) +\n",
    "                'row/col: 0~{}:\\n'.format(BOARD_NROWS - 1))\n",
    "\n",
    "            if not input_position_re.match(input_position):\n",
    "                print('Input position style is incorrect!\\n')\n",
    "                continue\n",
    "\n",
    "            input_position = tuple([int(x) for x in input_position.split(',')])\n",
    "            if input_position in positions:\n",
    "                break\n",
    "            else:\n",
    "                print('Input position was occupied, please input \"row,col\" again!\\n')\n",
    "\n",
    "        (r, c) = input_position\n",
    "        return r, c, self.symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Environment()\n",
    "# print(env.steps_left, env.is_done())\n",
    "# env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent1 = Agent(player='X', step_size=0.01, epsilon=0)\n",
    "# agent1.reset_episode(env)\n",
    "# agent1.load_state_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent1.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent1.V['0,0,0,0,0,0,0,0,1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human = Human(player='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1, c1, symbol1 = agent1.play(env)\n",
    "# env = env.step(r1, c1, symbol1)\n",
    "# print(env.steps_left, env.is_done())\n",
    "# env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2, c2, symbol2 = human.play(env)\n",
    "# env = env.step(r2, c2, symbol2)\n",
    "# print(env.steps_left, env.is_done())\n",
    "# env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1, c1, symbol1 = agent1.play(env)\n",
    "# env = env.step(r1, c1, symbol1)\n",
    "# print(env.steps_left, env.is_done())\n",
    "# env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2, c2, symbol2 = human.play(env)\n",
    "# env = env.step(r2, c2, symbol2)\n",
    "# print(env.steps_left, env.is_done())\n",
    "# env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1, c1, symbol1 = agent1.play(env)\n",
    "# env = env.step(r1, c1, symbol1)\n",
    "# print(env.steps_left, env.is_done())\n",
    "# env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_agent_compete():\n",
    "    \"\"\"Human compete with agent.\"\"\"\n",
    "    # Get human player.\n",
    "    human_name = input('Please input your name:\\n')\n",
    "    while True:\n",
    "        human_player = input('Please input your player: ' + \n",
    "                             '1st player (X), 2nd player (O):\\n')\n",
    "        if human_player in ['X', 'O']:\n",
    "            break\n",
    "\n",
    "    env = Environment()\n",
    "    env.show_board()\n",
    "    print('---')\n",
    "\n",
    "    # Set up human & agent as player1 or player2.\n",
    "    if human_player == 'X':\n",
    "        # Player1: human, player2: agent.\n",
    "        human, agent = Human(player='X'), Agent(player='O', epsilon=0)\n",
    "        player1, player2 = human, agent\n",
    "        player1_name, player2_name = human_name, 'Robot'\n",
    "    else:\n",
    "        # Player1: agent, player2: human.\n",
    "        agent, human = Agent(player='X', epsilon=0), Human(player='O')\n",
    "        player1, player2 = agent, human\n",
    "        player1_name, player2_name = 'Robot', human_name\n",
    "    agent.reset_episode(env)\n",
    "    agent.load_state_values()\n",
    "\n",
    "    # Start competition.\n",
    "    while not env.is_done():\n",
    "        # Player1 plays one step.\n",
    "        r1, c1, symbol1 = player1.play(env)\n",
    "        env = env.step(r1, c1, symbol1)\n",
    "        print('Player1, {} ({}), puts ({}, {})'\n",
    "              .format(player1_name, player1.player, r1, c1))\n",
    "        env.show_board()\n",
    "        print('---')\n",
    "\n",
    "        if env.is_done():\n",
    "            break\n",
    "\n",
    "        # Player2 plays the next step.\n",
    "        r2, c2, symbol2 = player2.play(env)\n",
    "        env = env.step(r2, c2, symbol2)\n",
    "        print('Player2, {} ({}), puts ({}, {})'\n",
    "              .format(player2_name, player2.player, r2, c2))\n",
    "        env.show_board()\n",
    "        print('---')\n",
    "\n",
    "    # Judge the winner.\n",
    "    if env.winner == human.symbol:\n",
    "        print('Congrats {}, you win!'.format(human_name))\n",
    "    elif env.winner == -human.symbol:\n",
    "        print('{} loses to Robot...'.format(human_name))\n",
    "    else:\n",
    "        print('{} and Robot tie.'.format(human_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your name:\n",
      "Bowen\n",
      "Please input your player: 1st player (X), 2nd player (O):\n",
      "O\n",
      "Board: is_done=False, steps_left=9, winner=0\n",
      "[' ', ' ', ' ']\n",
      "[' ', ' ', ' ']\n",
      "[' ', ' ', ' ']\n",
      "---\n",
      "Player1, Robot (X), puts (1, 1)\n",
      "Board: is_done=False, steps_left=8, winner=0\n",
      "[' ', ' ', ' ']\n",
      "[' ', 'X', ' ']\n",
      "[' ', ' ', ' ']\n",
      "---\n",
      "Please input position for O in the format: \"row,col\" with row/col: 0~2:\n",
      "0,0\n",
      "Player2, Bowen (O), puts (0, 0)\n",
      "Board: is_done=False, steps_left=7, winner=0\n",
      "['O', ' ', ' ']\n",
      "[' ', 'X', ' ']\n",
      "[' ', ' ', ' ']\n",
      "---\n",
      "Player1, Robot (X), puts (1, 2)\n",
      "Board: is_done=False, steps_left=6, winner=0\n",
      "['O', ' ', ' ']\n",
      "[' ', 'X', 'X']\n",
      "[' ', ' ', ' ']\n",
      "---\n",
      "Please input position for O in the format: \"row,col\" with row/col: 0~2:\n",
      "1m0\n",
      "Input position style is incorrect!\n",
      "\n",
      "Please input position for O in the format: \"row,col\" with row/col: 0~2:\n",
      "1,0\n",
      "Player2, Bowen (O), puts (1, 0)\n",
      "Board: is_done=False, steps_left=5, winner=0\n",
      "['O', ' ', ' ']\n",
      "['O', 'X', 'X']\n",
      "[' ', ' ', ' ']\n",
      "---\n",
      "Player1, Robot (X), puts (2, 0)\n",
      "Board: is_done=False, steps_left=4, winner=0\n",
      "['O', ' ', ' ']\n",
      "['O', 'X', 'X']\n",
      "['X', ' ', ' ']\n",
      "---\n",
      "Please input position for O in the format: \"row,col\" with row/col: 0~2:\n",
      "0,2\n",
      "Player2, Bowen (O), puts (0, 2)\n",
      "Board: is_done=False, steps_left=3, winner=0\n",
      "['O', ' ', 'O']\n",
      "['O', 'X', 'X']\n",
      "['X', ' ', ' ']\n",
      "---\n",
      "Player1, Robot (X), puts (0, 1)\n",
      "Board: is_done=False, steps_left=2, winner=0\n",
      "['O', 'X', 'O']\n",
      "['O', 'X', 'X']\n",
      "['X', ' ', ' ']\n",
      "---\n",
      "Please input position for O in the format: \"row,col\" with row/col: 0~2:\n",
      "2,1\n",
      "Player2, Bowen (O), puts (2, 1)\n",
      "Board: is_done=False, steps_left=1, winner=0\n",
      "['O', 'X', 'O']\n",
      "['O', 'X', 'X']\n",
      "['X', 'O', ' ']\n",
      "---\n",
      "Player1, Robot (X), puts (2, 2)\n",
      "Board: is_done=True, steps_left=0, winner=0\n",
      "['O', 'X', 'O']\n",
      "['O', 'X', 'X']\n",
      "['X', 'O', 'X']\n",
      "---\n",
      "Bowen and Robot tie.\n"
     ]
    }
   ],
   "source": [
    "# env, human, agent = human_agent_compete()\n",
    "human_agent_compete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
